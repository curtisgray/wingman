[
    {
        "files": [
            {
                "filename": "guanaco-3b-uncensored-v2.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "guanaco-3b-uncensored-v2.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "guanaco-3b-uncensored-v2.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "guanaco-3b-uncensored-v2.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "guanaco-3b-uncensored-v2.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Guanaco-3B-Uncensored-v2-GGML",
        "modelId": "Guanaco-3B-Uncensored-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "openbuddy-llama2-13b-v11.1.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/OpenBuddy-Llama2-13B-v11.1-GGML",
        "modelId": "OpenBuddy-Llama2-13B-v11.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "asclepius-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "asclepius-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Asclepius-13B-GGML",
        "modelId": "Asclepius-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "speechless-llama2-hermes-orca-platypus-wizardlm-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GGML",
        "modelId": "Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "speechless-llama2-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Speechless-Llama2-13B-GGML",
        "modelId": "Speechless-Llama2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "undimix-v2-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "undimix-v2-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/UndiMix-v2-13B-GGML",
        "modelId": "UndiMix-v2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "undimix-v1-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "undimix-v1-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/UndiMix-v1-13B-GGML",
        "modelId": "UndiMix-v1-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "stheno-l2-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "stheno-l2-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Stheno-L2-13B-GGML",
        "modelId": "Stheno-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "stheno-inverted-l2-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Stheno-Inverted-L2-13B-GGML",
        "modelId": "Stheno-Inverted-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "airoboros-33b-2.1.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Airoboros-33B-2.1-GGML",
        "modelId": "Airoboros-33B-2.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "yarn-llama-2-13b-64k.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Yarn-Llama-2-13B-64K-GGML",
        "modelId": "Yarn-Llama-2-13B-64K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "yarn-llama-2-13b-128k.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Yarn-Llama-2-13B-128K-GGML",
        "modelId": "Yarn-Llama-2-13B-128K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "yarn-llama-2-7b-64k.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Yarn-Llama-2-7B-64K-GGML",
        "modelId": "Yarn-Llama-2-7B-64K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "yarn-llama-2-7b-128k.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Yarn-Llama-2-7B-128K-GGML",
        "modelId": "Yarn-Llama-2-7B-128K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "synthia-70b-v1.1.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Synthia-70B-v1.1-GGML",
        "modelId": "Synthia-70B-v1.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "llama-2-13b-chat-limarp-v2-merged.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/llama-2-13B-chat-limarp-v2-merged-GGML",
        "modelId": "llama-2-13B-chat-limarp-v2-merged-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "lokus-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "lokus-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/LoKuS-13B-GGML",
        "modelId": "LoKuS-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "mythomax-l2-kimiko-v2-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/MythoMax-L2-Kimiko-v2-13B-GGML",
        "modelId": "MythoMax-L2-Kimiko-v2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "airoboros-l2-70b-2.1-creative.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Airoboros-L2-70B-2.1-Creative-GGML",
        "modelId": "Airoboros-L2-70B-2.1-Creative-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "fiction.live-Kimiko-V2-70B.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/fiction.live-Kimiko-V2-70B-GGML",
        "modelId": "fiction.live-Kimiko-V2-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "kimiko-v2-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Kimiko-v2-13B-GGML",
        "modelId": "Kimiko-v2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "athena-v1.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "athena-v1.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "athena-v1.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "athena-v1.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "athena-v1.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "athena-v1.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "athena-v1.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "athena-v1.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "athena-v1.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "athena-v1.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "athena-v1.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "athena-v1.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "athena-v1.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "athena-v1.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Athena-v1-GGML",
        "modelId": "Athena-v1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "luban-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "luban-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "luban-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "luban-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "luban-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "luban-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "luban-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "luban-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "luban-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "luban-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "luban-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "luban-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "luban-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "luban-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Luban-13B-GGML",
        "modelId": "Luban-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "huginn-13b-v4.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "huginn-13b-v4.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Huginn-13B-v4-GGML",
        "modelId": "Huginn-13B-v4-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "huginn-13b-v4.5.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Huginn-13B-v4.5-GGML",
        "modelId": "Huginn-13B-v4.5-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "mythical-destroyer-v2-l2-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Mythical-Destroyer-V2-L2-13B-GGML",
        "modelId": "Mythical-Destroyer-V2-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "airoboros-l2-7b-2.1.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Airoboros-L2-7B-2.1-GGML",
        "modelId": "Airoboros-L2-7B-2.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "model_007-70b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "model_007-70b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "model_007-70b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "model_007-70b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "model_007-70b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "model_007-70b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "model_007-70b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "model_007-70b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "model_007-70b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "model_007-70b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "model_007-70b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/model_007-70B-GGML",
        "modelId": "model_007-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "mythical-destroyer-l2-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Mythical-Destroyer-L2-13B-GGML",
        "modelId": "Mythical-Destroyer-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "airoboros-l2-13b-2.1.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Airoboros-L2-13B-2.1-GGML",
        "modelId": "Airoboros-L2-13B-2.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "mythomax-kimiko-mix.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/MythoMax-Kimiko-Mix-GGML",
        "modelId": "MythoMax-Kimiko-Mix-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "samantha-1.11-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Samantha-1.11-13B-GGML",
        "modelId": "Samantha-1.11-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "lemur-70b-chat-v1.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Lemur-70B-Chat-v1-GGML",
        "modelId": "Lemur-70B-Chat-v1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "wizardcoder-python-13b-v1.0.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/WizardCoder-Python-13B-V1.0-GGML",
        "modelId": "WizardCoder-Python-13B-V1.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "codellama-13b-oasst-sft-v10.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/CodeLlama-13B-oasst-sft-v10-GGML",
        "modelId": "CodeLlama-13B-oasst-sft-v10-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "huginn-22b-prototype.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Huginn-22B-Prototype-GGML",
        "modelId": "Huginn-22B-Prototype-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "airoboros-l2-70b-2.1.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Airoboros-L2-70B-2.1-GGML",
        "modelId": "Airoboros-L2-70B-2.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "llama-2-70b-orca-200k.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Llama-2-70B-Orca-200k-GGML",
        "modelId": "Llama-2-70B-Orca-200k-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "genz-70b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "genz-70b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "genz-70b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "genz-70b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "genz-70b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "genz-70b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "genz-70b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "genz-70b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "genz-70b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "genz-70b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "genz-70b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Genz-70b-GGML",
        "modelId": "Genz-70b-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "zarafusionex-1.1-l2-7b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Zarafusionex-1.1-L2-7B-GGML",
        "modelId": "Zarafusionex-1.1-L2-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "synthia-70b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "synthia-70b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "synthia-70b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "synthia-70b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "synthia-70b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "synthia-70b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "synthia-70b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "synthia-70b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "synthia-70b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "synthia-70b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "synthia-70b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Synthia-70B-GGML",
        "modelId": "Synthia-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "llama2-70b-oasst-sft-v10.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Llama2-70B-OASST-SFT-v10-GGML",
        "modelId": "Llama2-70B-OASST-SFT-v10-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "codellama-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "codellama-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/CodeLlama-13B-GGML",
        "modelId": "CodeLlama-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "samantha-1.11-70b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Samantha-1.11-70B-GGML",
        "modelId": "Samantha-1.11-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "codellama-13b-python.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "codellama-13b-python.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/CodeLlama-13B-Python-GGML",
        "modelId": "CodeLlama-13B-Python-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "codellama-13b-instruct.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/CodeLlama-13B-Instruct-GGML",
        "modelId": "CodeLlama-13B-Instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "codellama-7b-python.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "codellama-7b-python.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/CodeLlama-7B-Python-GGML",
        "modelId": "CodeLlama-7B-Python-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "codellama-7b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "codellama-7b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/CodeLlama-7B-GGML",
        "modelId": "CodeLlama-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "codellama-7b-instruct.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/CodeLlama-7B-Instruct-GGML",
        "modelId": "CodeLlama-7B-Instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "nous-puffin-70b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "nous-puffin-70b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "nous-puffin-70b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "nous-puffin-70b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "nous-puffin-70b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "nous-puffin-70b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "nous-puffin-70b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "nous-puffin-70b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "nous-puffin-70b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "nous-puffin-70b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "nous-puffin-70b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Nous-Puffin-70B-GGML",
        "modelId": "Nous-Puffin-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "nous-hermes-llama2-70b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            }
        ],
        "id": "TheBloke/Nous-Hermes-Llama2-70B-GGML",
        "modelId": "Nous-Hermes-Llama2-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "puddlejumper-13b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "puddlejumper-13b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/PuddleJumper-13B-GGML",
        "modelId": "PuddleJumper-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "trurl-2-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "trurl-2-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Trurl-2-7B-GGML",
        "modelId": "Trurl-2-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "trurl-2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "trurl-2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Trurl-2-13B-GGML",
        "modelId": "Trurl-2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "synthia-7b.ggmlv3.Q2_K.bin",
                "quantization": "Q2_K"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q3_K_L.bin",
                "quantization": "Q3_K_L"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q3_K_M.bin",
                "quantization": "Q3_K_M"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q3_K_S.bin",
                "quantization": "Q3_K_S"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q4_0.bin",
                "quantization": "Q4_0"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q4_1.bin",
                "quantization": "Q4_1"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q4_K_M.bin",
                "quantization": "Q4_K_M"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q4_K_S.bin",
                "quantization": "Q4_K_S"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q5_0.bin",
                "quantization": "Q5_0"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q5_1.bin",
                "quantization": "Q5_1"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q5_K_M.bin",
                "quantization": "Q5_K_M"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q5_K_S.bin",
                "quantization": "Q5_K_S"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q6_K.bin",
                "quantization": "Q6_K"
            },
            {
                "filename": "synthia-7b.ggmlv3.Q8_0.bin",
                "quantization": "Q8_0"
            }
        ],
        "id": "TheBloke/Synthia-7B-GGML",
        "modelId": "Synthia-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "zarablend-mx-l2-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Zarablend-MX-L2-7B-GGML",
        "modelId": "Zarablend-MX-L2-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "synthia-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "synthia-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "synthia-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "synthia-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "synthia-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "synthia-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "synthia-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "synthia-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "synthia-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "synthia-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "synthia-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "synthia-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "synthia-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "synthia-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Synthia-13B-GGML",
        "modelId": "Synthia-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "puma-3b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "puma-3b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "puma-3b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "puma-3b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "puma-3b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Puma-3b-GGML",
        "modelId": "Puma-3b-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "marx-3b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "marx-3b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "marx-3b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "marx-3b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "marx-3b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Marx-3b-GGML",
        "modelId": "Marx-3b-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "griffin-3b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "griffin-3b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "griffin-3b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "griffin-3b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "griffin-3b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Griffin-3B-GGML",
        "modelId": "Griffin-3B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronorctypus-limarobormes-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Chronorctypus-Limarobormes-13b-GGML",
        "modelId": "Chronorctypus-Limarobormes-13b-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "everythinglm-13b-v2-16k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/EverythingLM-13b-V2-16K-GGML",
        "modelId": "EverythingLM-13b-V2-16K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama2-22b-gplatty.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Llama2-22B-GPLATTY-GGML",
        "modelId": "Llama2-22B-GPLATTY-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "l2-mythomax22b-instruct-Falseblock.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/L2-MythoMax22b-Instruct-Falseblock-GGML",
        "modelId": "L2-MythoMax22b-Instruct-Falseblock-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-2-7b-32k-instruct.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Llama-2-7B-32K-Instruct-GGML",
        "modelId": "Llama-2-7B-32K-Instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llongorca-13b-16k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llongorca-13b-16k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/LlongOrca-13B-16K-GGML",
        "modelId": "LlongOrca-13B-16K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-1.1-70b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "samantha-1.1-70b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "samantha-1.1-70b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "samantha-1.1-70b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "samantha-1.1-70b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "samantha-1.1-70b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "samantha-1.1-70b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "samantha-1.1-70b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "samantha-1.1-70b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "samantha-1.1-70b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "samantha-1.1-70b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Samantha-1.1-70B-GGML",
        "modelId": "Samantha-1.1-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "zarablend-l2-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "zarablend-l2-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Zarablend-L2-7B-GGML",
        "modelId": "Zarablend-L2-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "octocoder.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "octocoder.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "octocoder.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "octocoder.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "octocoder.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Octocoder-GGML",
        "modelId": "Octocoder-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama2-13b-megacode2-oasst.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Llama2-13B-MegaCode2-OASST-GGML",
        "modelId": "Llama2-13B-MegaCode2-OASST-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "orca_mini_v3_70b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/orca_mini_v3_70B-GGML",
        "modelId": "orca_mini_v3_70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "carl-33b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "carl-33b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "carl-33b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "carl-33b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "carl-33b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "carl-33b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "carl-33b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "carl-33b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "carl-33b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "carl-33b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "carl-33b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "carl-33b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "carl-33b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "carl-33b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Carl-33B-GGML",
        "modelId": "Carl-33B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "carl-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "carl-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "carl-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "carl-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "carl-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "carl-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "carl-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "carl-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "carl-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "carl-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "carl-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "carl-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "carl-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "carl-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Carl-13B-GGML",
        "modelId": "Carl-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "scarlett-33b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "scarlett-33b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/scarlett-33B-GGML",
        "modelId": "scarlett-33B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "scarlett-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "scarlett-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Scarlett-13B-GGML",
        "modelId": "Scarlett-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "scarlett-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "scarlett-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Scarlett-7B-GGML",
        "modelId": "Scarlett-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama2-22b-daydreamer-v3.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Llama2-22B-Daydreamer-v3-GGML",
        "modelId": "Llama2-22B-Daydreamer-v3-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "losslessmegacoder-llama2-13b-min.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/LosslessMegaCoder-Llama2-13B-Mini-GGML",
        "modelId": "LosslessMegaCoder-Llama2-13B-Mini-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "godzilla2-70b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "godzilla2-70b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "godzilla2-70b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "godzilla2-70b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "godzilla2-70b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "godzilla2-70b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "godzilla2-70b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "godzilla2-70b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "godzilla2-70b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "godzilla2-70b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "godzilla2-70b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/GodziLLa2-70B-GGML",
        "modelId": "GodziLLa2-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "puli-gpt-3sx.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "puli-gpt-3sx.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "puli-gpt-3sx.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "puli-gpt-3sx.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "puli-gpt-3sx.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/PULI-GPT-3SX-GGML",
        "modelId": "PULI-GPT-3SX-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama2-22b-daydreamer-v2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/llama2-22B-daydreamer-v2-GGML",
        "modelId": "llama2-22B-daydreamer-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "codeup-alpha-13b-hf.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/CodeUp-Alpha-13B-HF-GGML",
        "modelId": "CodeUp-Alpha-13B-HF-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "losslessmegacoder-llama2-7b-mini.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/LosslessMegaCoder-Llama2-7B-Mini-GGML",
        "modelId": "LosslessMegaCoder-Llama2-7B-Mini-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "huginn-v3-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "huginn-v3-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Huginn-v3-13B-GGML",
        "modelId": "Huginn-v3-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llongorca-7b-16k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llongorca-7b-16k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/LlongOrca-7B-16K-GGML",
        "modelId": "LlongOrca-7B-16K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "everythinglm-13b-16k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/EverythingLM-13B-16K-GGML",
        "modelId": "EverythingLM-13B-16K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "openorca-platypus2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/OpenOrca-Platypus2-13B-GGML",
        "modelId": "OpenOrca-Platypus2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardmath-70b-v1.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/WizardMath-70B-V1.0-GGML",
        "modelId": "WizardMath-70B-V1.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizardmath-13b-v1.0.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardMath-13B-V1.0-GGML",
        "modelId": "WizardMath-13B-V1.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizardmath-7b-v1.0.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardMath-7B-V1.0-GGML",
        "modelId": "WizardMath-7B-V1.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mythomax-l2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "mythomax-l2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/MythoMax-L2-13B-GGML",
        "modelId": "MythoMax-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "camel-platypus2-70b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "camel-platypus2-70b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "camel-platypus2-70b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "camel-platypus2-70b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "camel-platypus2-70b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "camel-platypus2-70b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "camel-platypus2-70b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "camel-platypus2-70b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "camel-platypus2-70b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "camel-platypus2-70b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "camel-platypus2-70b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Camel-Platypus2-70B-GGML",
        "modelId": "Camel-Platypus2-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "platypus2-70b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "platypus2-70b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "platypus2-70b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "platypus2-70b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "platypus2-70b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "platypus2-70b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "platypus2-70b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "platypus2-70b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "platypus2-70b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "platypus2-70b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "platypus2-70b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Platypus2-70B-GGML",
        "modelId": "Platypus2-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "platypus2-70b-instruct.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Platypus2-70B-Instruct-GGML",
        "modelId": "Platypus2-70B-Instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "camel-platypus2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "camel-platypus2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Camel-Platypus2-13B-GGML",
        "modelId": "Camel-Platypus2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "platypus2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "platypus2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Platypus2-13B-GGML",
        "modelId": "Platypus2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "orca_mini_v3_13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/orca_mini_v3_13B-GGML",
        "modelId": "orca_mini_v3_13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "stable-platypus2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "stable-platypus2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Stable-Platypus2-13B-GGML",
        "modelId": "Stable-Platypus2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "orca_mini_v3_7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/orca_mini_v3_7B-GGML",
        "modelId": "orca_mini_v3_7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "alpacacielo2-7b-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/AlpacaCielo2-7B-8K-GGML",
        "modelId": "AlpacaCielo2-7B-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "huginnv1.2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "huginnv1.2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/huginnv1.2-GGML",
        "modelId": "huginnv1.2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-70b-v1.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/WizardLM-70B-V1.0-GGML",
        "modelId": "WizardLM-70B-V1.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "firefly-llama2-13b-v1.2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Firefly-Llama2-13B-v1.2-GGML",
        "modelId": "Firefly-Llama2-13B-v1.2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "stablecode-instruct-alpha-3b.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "stablecode-instruct-alpha-3b.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "stablecode-instruct-alpha-3b.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "stablecode-instruct-alpha-3b.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "stablecode-instruct-alpha-3b.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/stablecode-instruct-alpha-3b-GGML",
        "modelId": "stablecode-instruct-alpha-3b-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "stablecode-completion-alpha-3b-4k.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "stablecode-completion-alpha-3b-4k.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "stablecode-completion-alpha-3b-4k.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "stablecode-completion-alpha-3b-4k.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "stablecode-completion-alpha-3b-4k.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/stablecode-completion-alpha-3b-4k-GGML",
        "modelId": "stablecode-completion-alpha-3b-4k-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mythomix-l2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "mythomix-l2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/MythoMix-L2-13B-GGML",
        "modelId": "MythoMix-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "spring-dragon.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "spring-dragon.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "spring-dragon.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "spring-dragon.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "spring-dragon.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "spring-dragon.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "spring-dragon.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "spring-dragon.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "spring-dragon.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "spring-dragon.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "spring-dragon.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "spring-dragon.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "spring-dragon.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "spring-dragon.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Spring-Dragon-GGML",
        "modelId": "Spring-Dragon-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "dolphin-llama2-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Dolphin-Llama2-7B-GGML",
        "modelId": "Dolphin-Llama2-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizardlm-1.0-uncensored-llama2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GGML",
        "modelId": "WizardLM-1.0-Uncensored-Llama2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "huggin-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "huggin-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "huggin-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "huggin-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "huggin-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "huggin-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "huggin-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "huggin-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "huggin-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "huggin-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "huggin-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "huggin-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "huggin-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "huggin-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Huginn-13B-GGML",
        "modelId": "Huginn-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-70b-oasst-1-200.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Llama-2-70B-OASST-1-200-GGML",
        "modelId": "Llama-2-70B-OASST-1-200-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronos-beluga-v2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Chronos-Beluga-v2-13B-GGML",
        "modelId": "Chronos-Beluga-v2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "hermeslimarp-l2-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/HermesLimaRP-L2-7B-GGML",
        "modelId": "HermesLimaRP-L2-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-2.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Airoboros-65B-GPT4-2.0-GGML",
        "modelId": "Airoboros-65B-GPT4-2.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airolima-chronos-grad-l2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Airolima-Chronos-Grad-L2-13B-GGML",
        "modelId": "Airolima-Chronos-Grad-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronolima-airo-grad-l2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Chronolima-Airo-Grad-L2-13B-GGML",
        "modelId": "Chronolima-Airo-Grad-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-m2.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Airoboros-65B-GPT4-m2.0-GGML",
        "modelId": "Airoboros-65B-GPT4-m2.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "13b-legerdemain-l2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/13B-Legerdemain-L2-GGML",
        "modelId": "13B-Legerdemain-L2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "qcammel-13.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "qcammel-13.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "qcammel-13.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "qcammel-13.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "qcammel-13.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "qcammel-13.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "qcammel-13.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "qcammel-13.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "qcammel-13.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "qcammel-13.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "qcammel-13.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "qcammel-13.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "qcammel-13.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "qcammel-13.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/qCammel-13-GGML",
        "modelId": "qCammel-13-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronoboros-grad-l2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Chronoboros-Grad-L2-13B-GGML",
        "modelId": "Chronoboros-Grad-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-m2.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Airoboros-L2-70B-GPT4-m2.0-GGML",
        "modelId": "Airoboros-L2-70B-GPT4-m2.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronohermes-grad-l2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Chronohermes-Grad-L2-13B-GGML",
        "modelId": "Chronohermes-Grad-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mythologic-l2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "mythologic-l2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/MythoLogic-L2-13B-GGML",
        "modelId": "MythoLogic-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airochronos-l2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airochronos-l2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Airochronos-L2-13B-GGML",
        "modelId": "Airochronos-L2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vigogne-2-7b-chat.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Vigogne-2-7B-Chat-GGML",
        "modelId": "Vigogne-2-7B-Chat-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama2_70b_chat_uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/llama2_70b_chat_uncensored-GGML",
        "modelId": "llama2_70b_chat_uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronos-hermes-13b-v2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Chronos-Hermes-13B-v2-GGML",
        "modelId": "Chronos-Hermes-13B-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-13b-v1.5.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/vicuna-13B-v1.5-GGML",
        "modelId": "vicuna-13B-v1.5-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "qcammel-70-x.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "qcammel-70-x.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "qcammel-70-x.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "qcammel-70-x.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "qcammel-70-x.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "qcammel-70-x.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "qcammel-70-x.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "qcammel-70-x.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "qcammel-70-x.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "qcammel-70-x.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "qcammel-70-x.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/qCammel-70-x-GGML",
        "modelId": "qCammel-70-x-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-7b-v1.5.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/vicuna-7B-v1.5-GGML",
        "modelId": "vicuna-7B-v1.5-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "openorcaxopenchat-preview2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/OpenOrcaxOpenChat-Preview2-13B-GGML",
        "modelId": "OpenOrcaxOpenChat-Preview2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-7b-v1.5-16k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/vicuna-7B-v1.5-16K-GGML",
        "modelId": "vicuna-7B-v1.5-16K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronos-13b-v2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronos-13b-v2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Chronos-13B-v2-GGML",
        "modelId": "Chronos-13B-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-13b-v1.5-16k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/vicuna-13B-v1.5-16K-GGML",
        "modelId": "vicuna-13B-v1.5-16K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "hermes-llongma-2-13b-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Hermes-LLongMA-2-13B-8K-GGML",
        "modelId": "Hermes-LLongMA-2-13B-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "hermes-llongma-2-7b-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Hermes-LLongMA-2-7B-8K-GGML",
        "modelId": "Hermes-LLongMA-2-7B-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-2.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/airoboros-l2-70B-GPT4-2.0-GGML",
        "modelId": "airoboros-l2-70B-GPT4-2.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "openassistant-llama2-13b-orca-v2-8k-3166.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML",
        "modelId": "OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "codeup-llama-2-13b-chat-hf.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/CodeUp-Llama-2-13B-Chat-HF-GGML",
        "modelId": "CodeUp-Llama-2-13B-Chat-HF-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "newhope.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "newhope.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "newhope.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "newhope.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "newhope.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "newhope.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "newhope.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "newhope.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "newhope.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "newhope.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "newhope.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "newhope.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "newhope.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "newhope.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/NewHope-GGML",
        "modelId": "NewHope-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-33b-gpt4-2.0.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-33B-GPT4-2.0-GGML",
        "modelId": "airoboros-33B-GPT4-2.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-m2.0.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-l2-7B-gpt4-m2.0-GGML",
        "modelId": "airoboros-l2-7B-gpt4-m2.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-m2.0.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-l2-13b-gpt4-m2.0-GGML",
        "modelId": "airoboros-l2-13b-gpt4-m2.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-2.0.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-l2-7B-gpt4-2.0-GGML",
        "modelId": "airoboros-l2-7B-gpt4-2.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-2.0.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-l2-13b-gpt4-2.0-GGML",
        "modelId": "airoboros-l2-13b-gpt4-2.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-33b-gpt4-m2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-33B-GPT4-m2.0-GGML",
        "modelId": "airoboros-33B-GPT4-m2.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "openchat_v3.2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "openchat_v3.2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/OpenChat_v3.2-GGML",
        "modelId": "OpenChat_v3.2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "upstage-llama-2-70b-instruct-v2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Upstage-Llama-2-70B-instruct-v2-GGML",
        "modelId": "Upstage-Llama-2-70B-instruct-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vigogne-2-7b-instruct.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Vigogne-2-7B-Instruct-GGML",
        "modelId": "Vigogne-2-7B-Instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vigogne-2-13b-instruct.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Vigogne-2-13B-Instruct-GGML",
        "modelId": "Vigogne-2-13B-Instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "stablebeluga-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "stablebeluga-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/StableBeluga-7B-GGML",
        "modelId": "StableBeluga-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mythologic-mini-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "mythologic-mini-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/MythoLogic-Mini-7B-GGML",
        "modelId": "MythoLogic-Mini-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-70b-guanaco-qlora.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/llama-2-70b-Guanaco-QLoRA-GGML",
        "modelId": "llama-2-70b-Guanaco-QLoRA-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "stablebeluga-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "stablebeluga-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/StableBeluga-13B-GGML",
        "modelId": "StableBeluga-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-l2-70b-gpt4-1.4.1.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/airoboros-l2-70B-gpt4-1.4.1-GGML",
        "modelId": "airoboros-l2-70B-gpt4-1.4.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "kimiko-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "kimiko-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Kimiko-7B-GGML",
        "modelId": "Kimiko-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "stablebeluga2-70b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "stablebeluga2-70b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "stablebeluga2-70b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "stablebeluga2-70b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "stablebeluga2-70b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "stablebeluga2-70b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "stablebeluga2-70b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "stablebeluga2-70b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "stablebeluga2-70b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "stablebeluga2-70b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "stablebeluga2-70b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/StableBeluga2-70B-GGML",
        "modelId": "StableBeluga2-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "nous-hermes-llama-2-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Nous-Hermes-Llama-2-7B-GGML",
        "modelId": "Nous-Hermes-Llama-2-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "kimiko-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "kimiko-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Kimiko-13B-GGML",
        "modelId": "Kimiko-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "openassistant-llama2-13b-orca-8k-3319.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GGML",
        "modelId": "OpenAssistant-Llama2-13B-Orca-8K-3319-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizardlm-13b-v1.2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-13B-V1.2-GGML",
        "modelId": "WizardLM-13B-V1.2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "alpacacielo-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "alpacacielo-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/AlpacaCielo-13B-GGML",
        "modelId": "AlpacaCielo-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-l2-7b-gpt4-1.4.1.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-l2-7b-gpt4-1.4.1-GGML",
        "modelId": "airoboros-l2-7b-gpt4-1.4.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-l2-13b-gpt4-1.4.1.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-l2-13B-gpt4-1.4.1-GGML",
        "modelId": "airoboros-l2-13B-gpt4-1.4.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-70b-chat.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-70b-chat.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-70b-chat.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-70b-chat.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-70b-chat.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-70b-chat.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-70b-chat.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-70b-chat.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-70b-chat.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-70b-chat.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-70b-chat.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Llama-2-70B-Chat-GGML",
        "modelId": "Llama-2-70B-Chat-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-2-7b-chat-codeCherryPop.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GGML",
        "modelId": "llama2-7b-chat-codeCherryPop-qLoRA-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama2_7b_chat_uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/llama2_7b_chat_uncensored-GGML",
        "modelId": "llama2_7b_chat_uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llongma-2-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llongma-2-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/LLongMA-2-7B-GGML",
        "modelId": "LLongMA-2-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "nous-hermes-llama2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Nous-Hermes-Llama2-GGML",
        "modelId": "Nous-Hermes-Llama2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-2-13b-german-assistant-v2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/llama-2-13B-German-Assistant-v2-GGML",
        "modelId": "llama-2-13B-German-Assistant-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-2-13b-guanaco-qlora.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/llama-2-13B-Guanaco-QLoRA-GGML",
        "modelId": "llama-2-13B-Guanaco-QLoRA-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-2-7b-guanaco-qlora.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/llama-2-7B-Guanaco-QLoRA-GGML",
        "modelId": "llama-2-7B-Guanaco-QLoRA-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "luna-ai-llama2-uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Luna-AI-Llama2-Uncensored-GGML",
        "modelId": "Luna-AI-Llama2-Uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "redmond-puffin-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "redmond-puffin-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Redmond-Puffin-13B-GGML",
        "modelId": "Redmond-Puffin-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-13b-chat.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-2-13b-chat.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Llama-2-13B-chat-GGML",
        "modelId": "Llama-2-13B-chat-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-7b-chat.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-2-7b-chat.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Llama-2-7B-Chat-GGML",
        "modelId": "Llama-2-7B-Chat-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Llama-2-13B-GGML",
        "modelId": "Llama-2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-2-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Llama-2-7B-GGML",
        "modelId": "Llama-2-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-33b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-33b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/vicuna-33B-GGML",
        "modelId": "vicuna-33B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "bigtrans-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "bigtrans-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/BigTranslate-13B-GGML",
        "modelId": "BigTranslate-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mythologic-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "mythologic-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/MythoLogic-13B-GGML",
        "modelId": "MythoLogic-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-2-70b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-2-70b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-2-70b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-2-70b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-2-70b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-2-70b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-2-70b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-2-70b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-2-70b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-2-70b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-2-70b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Llama-2-70B-GGML",
        "modelId": "Llama-2-70B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "dolphin-llama-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "dolphin-llama-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Dolphin-Llama-13B-GGML",
        "modelId": "Dolphin-Llama-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mythoboros-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "mythoboros-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/MythoBoros-13B-GGML",
        "modelId": "MythoBoros-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "upstage-llama-65b-instruct.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/Upstage-Llama1-65B-Instruct-GGML",
        "modelId": "Upstage-Llama1-65B-Instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "upstage-llama-30b-instruct-2048.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/upstage-llama-30b-instruct-2048-GGML",
        "modelId": "upstage-llama-30b-instruct-2048-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-13b-v1.3-german.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Vicuna-13B-v1.3-German-GGML",
        "modelId": "Vicuna-13B-v1.3-German-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "13b-bluemethod.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "13b-bluemethod.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/13B-BlueMethod-GGML",
        "modelId": "13B-BlueMethod-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "13b-ouroboros.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "13b-ouroboros.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/13B-Ouroboros-GGML",
        "modelId": "13B-Ouroboros-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "30b-epsilon.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "30b-epsilon.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/30B-Epsilon-GGML",
        "modelId": "30B-Epsilon-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "replit-openorca.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "replit-openorca.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "replit-openorca.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "replit-openorca.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "replit-openorca.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/replit-openorca-GGML",
        "modelId": "replit-openorca-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "guanaco-7B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "guanaco-7B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/guanaco-7B-GGML",
        "modelId": "guanaco-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "replit-code-instruct-glaive.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "replit-code-instruct-glaive.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "replit-code-instruct-glaive.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "replit-code-instruct-glaive.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "replit-code-instruct-glaive.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Replit-Code-Instruct-Glaive-GGML",
        "modelId": "Replit-Code-Instruct-Glaive-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "evol-replit-v1.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "evol-replit-v1.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "evol-replit-v1.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "evol-replit-v1.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "evol-replit-v1.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Evol-Replit-v1-GGML",
        "modelId": "Evol-Replit-v1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/LLaMa-7B-GGML",
        "modelId": "LLaMa-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-30b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-30b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-30b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-30b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-30b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-30b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-30b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-30b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-30b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-30b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-30b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-30b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-30b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-30b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/LLaMa-30B-GGML",
        "modelId": "LLaMa-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/LLaMa-13B-GGML",
        "modelId": "LLaMa-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-65b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-65b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-65b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-65b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-65b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-65b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-65b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-65b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-65b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-65b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-65b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-65b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/llama-65B-GGML",
        "modelId": "llama-65B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardcoder-guanaco-15b-v1.1.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardcoder-guanaco-15b-v1.1.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardcoder-guanaco-15b-v1.1.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardCoder-Guanaco-15B-V1.1-GGML",
        "modelId": "WizardCoder-Guanaco-15B-V1.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v3.ggccv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v3.ggccv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v3.ggccv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v3.ggccv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v3.ggccv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GGML",
        "modelId": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v3-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "openorca-preview1-200k-llama-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/OpenOrca-Preview1-13B-GGML",
        "modelId": "OpenOrca-Preview1-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "starcoderplus-guanaco-gpt4.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "starcoderplus-guanaco-gpt4.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "starcoderplus-guanaco-gpt4.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "starcoderplus-guanaco-gpt4.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "starcoderplus-guanaco-gpt4.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Starcoderplus-Guanaco-GPT4-15B-V1.0-GGML",
        "modelId": "Starcoderplus-Guanaco-GPT4-15B-V1.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "open-llama-7b-v2-open-instruct.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/open-llama-7B-v2-open-instruct-GGML",
        "modelId": "open-llama-7B-v2-open-instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airochronos-33b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airochronos-33b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airochronos-33B-GGML",
        "modelId": "airochronos-33B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mpt-30b-dolphin-v2.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mpt-30b-dolphin-v2.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mpt-30b-dolphin-v2.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mpt-30b-dolphin-v2.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mpt-30b-dolphin-v2.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/MPT-30B-Dolphin-v2-GGML",
        "modelId": "MPT-30B-Dolphin-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronoboros-33b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronoboros-33b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Chronoboros-33B-GGML",
        "modelId": "Chronoboros-33B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "godzilla-30b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "godzilla-30b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/GodziLLa-30B-GGML",
        "modelId": "GodziLLa-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardcoder-guanaco-15b-v1.0.ggmlv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardcoder-guanaco-15b-v1.0.ggmlv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardcoder-guanaco-15b-v1.0.ggmlv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardcoder-guanaco-15b-v1.0.ggmlv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardcoder-guanaco-15b-v1.0.ggmlv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardCoder-Guanaco-15B-V1.0-GGML",
        "modelId": "WizardCoder-Guanaco-15B-V1.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizardlm-13b-v1.1.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-13B-V1.1-GGML",
        "modelId": "WizardLM-13B-V1.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "orca_mini_v2_13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/orca_mini_v2_13b-GGML",
        "modelId": "orca_mini_v2_13b-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v2.ggccv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v2.ggccv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v2.ggccv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v2.ggccv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v2.ggccv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2-GGML",
        "modelId": "h2ogpt-gm-oasst1-en-2048-falcon-7b-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizard-falcon-7b.ggmlv3.fp16.bin",
                "quantization": "fp16"
            },
            {
                "filename": "wizardlm-7b-uncensored.ggccv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-7b-uncensored.ggccv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-7b-uncensored.ggccv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-7b-uncensored.ggccv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardlm-7b-uncensored.ggccv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-Uncensored-Falcon-7B-GGML",
        "modelId": "WizardLM-Uncensored-Falcon-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "falcon-7b-instruct.ggccv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "falcon-7b-instruct.ggccv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "falcon-7b-instruct.ggccv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "falcon-7b-instruct.ggccv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "falcon-7b-instruct.ggccv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/falcon-7b-instruct-GGML",
        "modelId": "falcon-7b-instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-13b-v1.1-superhot-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-13b-v1.1-superhot-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardlm-13b-v1.1-superhot-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML",
        "modelId": "WizardLM-13B-V1-1-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "guanaco-33b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "guanaco-33b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "guanaco-33b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "guanaco-33b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "guanaco-33b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "guanaco-33b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "guanaco-33b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "guanaco-33b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "guanaco-33b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Guanaco-33B-SuperHOT-8K-GGML",
        "modelId": "Guanaco-33B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "camel-13b-combined-data-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "camel-13b-combined-data-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "camel-13b-combined-data-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "camel-13b-combined-data-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "camel-13b-combined-data-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "camel-13b-combined-data-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "camel-13b-combined-data-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "camel-13b-combined-data-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "camel-13b-combined-data-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/CAMEL-13B-Combined-Data-SuperHOT-8K-GGML",
        "modelId": "CAMEL-13B-Combined-Data-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-13b-v1.0-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-13b-v1.0-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-13b-v1.0-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.0-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.0-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.0-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.0-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.0-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.0-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GGML",
        "modelId": "WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "pygmalion-7b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "pygmalion-7b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "pygmalion-7b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "pygmalion-7b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "pygmalion-7b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "pygmalion-7b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "pygmalion-7b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "pygmalion-7b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "pygmalion-7b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Pygmalion-7B-SuperHOT-8K-GGML",
        "modelId": "Pygmalion-7B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "falcon-40b-top1-560.ggccv1.q2_k.bin",
                "quantization": "q2_k"
            },
            {
                "filename": "falcon-40b-top1-560.ggccv1.q3_k.bin",
                "quantization": "q3_k"
            },
            {
                "filename": "falcon-40b-top1-560.ggccv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "falcon-40b-top1-560.ggccv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "falcon-40b-top1-560.ggccv1.q4_k.bin",
                "quantization": "q4_k"
            },
            {
                "filename": "falcon-40b-top1-560.ggccv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "falcon-40b-top1-560.ggccv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "falcon-40b-top1-560.ggccv1.q5_k.bin",
                "quantization": "q5_k"
            },
            {
                "filename": "falcon-40b-top1-560.ggccv1.q6_k.bin",
                "quantization": "q6_k"
            },
            {
                "filename": "falcon-40b-top1-560.ggccv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/falcon-40b-sft-top1-560-GGML",
        "modelId": "falcon-40b-sft-top1-560-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "h2ogpt-falcon-40b.ggmlv3.q2_k.bin",
                "quantization": "q2_k"
            },
            {
                "filename": "h2ogpt-falcon-40b.ggmlv3.q3_k.bin",
                "quantization": "q3_k"
            },
            {
                "filename": "h2ogpt-falcon-40b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "h2ogpt-falcon-40b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "h2ogpt-falcon-40b.ggmlv3.q4_k.bin",
                "quantization": "q4_k"
            },
            {
                "filename": "h2ogpt-falcon-40b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "h2ogpt-falcon-40b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "h2ogpt-falcon-40b.ggmlv3.q5_k.bin",
                "quantization": "q5_k"
            },
            {
                "filename": "h2ogpt-falcon-40b.ggmlv3.q6_k.bin",
                "quantization": "q6_k"
            },
            {
                "filename": "h2ogpt-falcon-40b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML",
        "modelId": "h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "falcon-40b-sft-mix-1226.ggccv1.q2_k.bin",
                "quantization": "q2_k"
            },
            {
                "filename": "falcon-40b-sft-mix-1226.ggccv1.q3_k.bin",
                "quantization": "q3_k"
            },
            {
                "filename": "falcon-40b-sft-mix-1226.ggccv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "falcon-40b-sft-mix-1226.ggccv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "falcon-40b-sft-mix-1226.ggccv1.q4_k.bin",
                "quantization": "q4_k"
            },
            {
                "filename": "falcon-40b-sft-mix-1226.ggccv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "falcon-40b-sft-mix-1226.ggccv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "falcon-40b-sft-mix-1226.ggccv1.q5_k.bin",
                "quantization": "q5_k"
            },
            {
                "filename": "falcon-40b-sft-mix-1226.ggccv1.q6_k.bin",
                "quantization": "q6_k"
            },
            {
                "filename": "falcon-40b-sft-mix-1226.ggccv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/falcon-40b-sft-mix-1226-GGML",
        "modelId": "falcon-40b-sft-mix-1226-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "pmc_llama-7b-10-epoch-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "pmc_llama-7b-10-epoch-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "pmc_llama-7b-10-epoch-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "pmc_llama-7b-10-epoch-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "pmc_llama-7b-10-epoch-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "pmc_llama-7b-10-epoch-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "pmc_llama-7b-10-epoch-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "pmc_llama-7b-10-epoch-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "pmc_llama-7b-10-epoch-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/PMC_LLAMA-7B-10-Epoch-SuperHOT-8K-GGML",
        "modelId": "PMC_LLAMA-7B-10-Epoch-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "tulu-7b-superhot-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "tulu-7b-superhot-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "tulu-7b-superhot-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "tulu-7b-superhot-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "tulu-7b-superhot-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Tulu-7B-SuperHOT-8K-GGML",
        "modelId": "Tulu-7B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "selfee-7b-superhot-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "selfee-7b-superhot-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "selfee-7b-superhot-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "selfee-7b-superhot-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "selfee-7b-superhot-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Selfee-7B-SuperHOT-8K-GGML",
        "modelId": "Selfee-7B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "selfee-13b-superhot-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "selfee-13b-superhot-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "selfee-13b-superhot-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "selfee-13b-superhot-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "selfee-13b-superhot-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Selfee-13B-SuperHOT-8K-GGML",
        "modelId": "Selfee-13B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-7b-v1.0-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-7b-v1.0-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-7b-v1.0-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-7b-v1.0-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-7b-v1.0-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-7b-v1.0-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-7b-v1.0-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-7b-v1.0-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardlm-7b-v1.0-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GGML",
        "modelId": "WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "longchat-7b-16k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "longchat-7b-16k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "longchat-7b-16k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "longchat-7b-16k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "longchat-7b-16k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "longchat-7b-16k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "longchat-7b-16k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "longchat-7b-16k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "longchat-7b-16k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/LongChat-7B-GGML",
        "modelId": "LongChat-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizard-vicuna-7b-uncensored-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizard-vicuna-7b-uncensored-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizard-vicuna-7b-uncensored-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizard-vicuna-7b-uncensored-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizard-vicuna-7b-uncensored-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizard-vicuna-7b-uncensored-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizard-vicuna-7b-uncensored-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizard-vicuna-7b-uncensored-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizard-vicuna-7b-uncensored-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GGML",
        "modelId": "Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-7b-cot-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-7b-cot-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-7b-cot-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-7b-cot-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-7b-cot-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-7b-cot-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-7b-cot-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-7b-cot-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-7b-cot-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Vicuna-7B-CoT-SuperHOT-8K-GGML",
        "modelId": "Vicuna-7B-CoT-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-7b-v1.3-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-7b-v1.3-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-7b-v1.3-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-7b-v1.3-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-7b-v1.3-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-7b-v1.3-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-7b-v1.3-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-7b-v1.3-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-7b-v1.3-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Vicuna-7B-v1-3-SuperHOT-8K-GGML",
        "modelId": "Vicuna-7B-v1-3-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-1.1-llama-7b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "samantha-1.1-llama-7b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "samantha-1.1-llama-7b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "samantha-1.1-llama-7b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "samantha-1.1-llama-7b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "samantha-1.1-llama-7b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "samantha-1.1-llama-7b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "samantha-1.1-llama-7b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "samantha-1.1-llama-7b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Samantha-1-1-Llama-7B-SuperHOT-8K-GGML",
        "modelId": "Samantha-1-1-Llama-7B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "robin-7b-v2-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "robin-7b-v2-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "robin-7b-v2-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "robin-7b-v2-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "robin-7b-v2-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "robin-7b-v2-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "robin-7b-v2-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "robin-7b-v2-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "robin-7b-v2-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Robin-7B-v2-SuperHOT-8K-GGML",
        "modelId": "Robin-7B-v2-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "koala-7b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "koala-7b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "koala-7b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "koala-7b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "koala-7b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "koala-7b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "koala-7b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "koala-7b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "koala-7b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Koala-7B-SuperHOT-8K-GGML",
        "modelId": "Koala-7B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "koala-13b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "koala-13b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "koala-13b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "koala-13b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "koala-13b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "koala-13b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "koala-13b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "koala-13b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "koala-13b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Koala-13B-SuperHOT-8K-GGML",
        "modelId": "Koala-13B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "guanaco-7b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "guanaco-7b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "guanaco-7b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "guanaco-7b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "guanaco-7b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "guanaco-7b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "guanaco-7b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "guanaco-7b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "guanaco-7b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Guanaco-7B-SuperHOT-8K-GGML",
        "modelId": "Guanaco-7B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "baize-7b-v2-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "baize-7b-v2-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "baize-7b-v2-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "baize-7b-v2-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "baize-7b-v2-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "baize-7b-v2-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "baize-7b-v2-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "baize-7b-v2-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "baize-7b-v2-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Baize-v2-7B-SuperHOT-8K-GGML",
        "modelId": "Baize-v2-7B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "baize-13b-v2-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "baize-13b-v2-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "baize-13b-v2-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "baize-13b-v2-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "baize-13b-v2-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "baize-13b-v2-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "baize-13b-v2-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "baize-13b-v2-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "baize-13b-v2-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Baize-v2-13B-SuperHOT-8K-GGML",
        "modelId": "Baize-v2-13B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-7b-gpt4-1.4-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Airoboros-7B-GPT4-1-4-SuperHOT-8K-GGML",
        "modelId": "Airoboros-7B-GPT4-1-4-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "CAMEL-33B-combined-data-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "CAMEL-33B-combined-data-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "CAMEL-33B-combined-data-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "CAMEL-33B-combined-data-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "CAMEL-33B-combined-data-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "CAMEL-33B-combined-data-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "CAMEL-33B-combined-data-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "CAMEL-33B-combined-data-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "CAMEL-33B-combined-data-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/CAMEL-33B-Combined-Data-SuperHOT-8K-GGML",
        "modelId": "CAMEL-33B-Combined-Data-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "falcon-40b-instruct.ggccv1.q2_k.bin",
                "quantization": "q2_k"
            },
            {
                "filename": "falcon-40b-instruct.ggccv1.q3_k.bin",
                "quantization": "q3_k"
            },
            {
                "filename": "falcon-40b-instruct.ggccv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "falcon-40b-instruct.ggccv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "falcon-40b-instruct.ggccv1.q4_k.bin",
                "quantization": "q4_k"
            },
            {
                "filename": "falcon-40b-instruct.ggccv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "falcon-40b-instruct.ggccv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "falcon-40b-instruct.ggccv1.q5_k.bin",
                "quantization": "q5_k"
            },
            {
                "filename": "falcon-40b-instruct.ggccv1.q6_k.bin",
                "quantization": "q6_k"
            },
            {
                "filename": "falcon-40b-instruct.ggccv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/falcon-40b-instruct-GGML",
        "modelId": "falcon-40b-instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-uncensored-falcon-40b.ggccv1.q2_k.bin",
                "quantization": "q2_k"
            },
            {
                "filename": "wizardlm-uncensored-falcon-40b.ggccv1.q3_k.bin",
                "quantization": "q3_k"
            },
            {
                "filename": "wizardlm-uncensored-falcon-40b.ggccv1.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-uncensored-falcon-40b.ggccv1.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-uncensored-falcon-40b.ggccv1.q4_k.bin",
                "quantization": "q4_k"
            },
            {
                "filename": "wizardlm-uncensored-falcon-40b.ggccv1.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-uncensored-falcon-40b.ggccv1.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardlm-uncensored-falcon-40b.ggccv1.q5_k.bin",
                "quantization": "q5_k"
            },
            {
                "filename": "wizardlm-uncensored-falcon-40b.ggccv1.q6_k.bin",
                "quantization": "q6_k"
            },
            {
                "filename": "wizardlm-uncensored-falcon-40b.ggccv1.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-Uncensored-Falcon-40B-GGML",
        "modelId": "WizardLM-Uncensored-Falcon-40B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizardlm-13b-v1.0-uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-13B-V1.0-Uncensored-GGML",
        "modelId": "WizardLM-13B-V1.0-Uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "orca-mini-v2_7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/orca_mini_v2_7B-GGML",
        "modelId": "orca_mini_v2_7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "superplatty-30b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "superplatty-30b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/SuperPlatty-30B-GGML",
        "modelId": "SuperPlatty-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "longchat-13b-16k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "longchat-13b-16k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "longchat-13b-16k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "longchat-13b-16k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "longchat-13b-16k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "longchat-13b-16k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "longchat-13b-16k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "longchat-13b-16k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "longchat-13b-16k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/LongChat-13B-GGML",
        "modelId": "LongChat-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "redmond-hermes-coder.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "redmond-hermes-coder.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "redmond-hermes-coder.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "redmond-hermes-coder.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "redmond-hermes-coder.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Redmond-Hermes-Coder-GGML",
        "modelId": "Redmond-Hermes-Coder-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronos-hermes-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronos-hermes-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronos-hermes-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronos-hermes-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronos-hermes-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/chronos-hermes-13B-GGML",
        "modelId": "chronos-hermes-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-33b-1.3-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-33b-1.3-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-33b-1.3-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-33b-1.3-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-33b-1.3-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-33b-1.3-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-33b-1.3-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-33b-1.3-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-33b-1.3-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GGML",
        "modelId": "Vicuna-33B-1-3-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "tulu-13b-superhot-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "tulu-13b-superhot-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "tulu-13b-superhot-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "tulu-13b-superhot-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "tulu-13b-superhot-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Tulu-13B-SuperHOT-8K-GGML",
        "modelId": "Tulu-13B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chinese-alpaca-33b-superhot-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chinese-alpaca-33b-superhot-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chinese-alpaca-33b-superhot-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chinese-alpaca-33b-superhot-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chinese-alpaca-33b-superhot-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Chinese-Alpaca-33B-SuperHOT-8K-GGML",
        "modelId": "Chinese-Alpaca-33B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mpt-7b-chat.ggmlv0.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mpt-7b-chat.ggmlv0.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mpt-7b-chat.ggmlv0.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mpt-7b-chat.ggmlv0.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mpt-7b-chat.ggmlv0.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/mpt-7b-chat-GGML",
        "modelId": "mpt-7b-chat-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "h2ogpt-research-oasst1-llama-65b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/h2ogpt-research-oasst1-llama-65B-GGML",
        "modelId": "h2ogpt-research-oasst1-llama-65B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "WizardLM-Uncensored-SuperCOT-StoryTelling-30b-superhot-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-StoryTelling-30b-superhot-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-StoryTelling-30b-superhot-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-StoryTelling-30b-superhot-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-StoryTelling-30b-superhot-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GGML",
        "modelId": "WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizard-vicuna-13b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizard-vicuna-13b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizard-vicuna-13b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizard-vicuna-13b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizard-vicuna-13b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizard-vicuna-13b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizard-vicuna-13b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizard-vicuna-13b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizard-vicuna-13b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/wizard-vicuna-13B-SuperHOT-8K-GGML",
        "modelId": "wizard-vicuna-13B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizard-vicuna-13b-uncensored-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizard-vicuna-13b-uncensored-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizard-vicuna-13b-uncensored-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizard-vicuna-13b-uncensored-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizard-vicuna-13b-uncensored-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizard-vicuna-13b-uncensored-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizard-vicuna-13b-uncensored-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizard-vicuna-13b-uncensored-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizard-vicuna-13b-uncensored-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GGML",
        "modelId": "Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-33b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "samantha-33b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "samantha-33b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "samantha-33b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "samantha-33b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "samantha-33b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "samantha-33b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "samantha-33b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "samantha-33b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Samantha-33B-SuperHOT-8K-GGML",
        "modelId": "Samantha-33B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-13b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "samantha-13b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "samantha-13b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "samantha-13b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "samantha-13b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "samantha-13b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "samantha-13b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "samantha-13b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "samantha-13b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Samantha-13B-SuperHOT-8K-GGML",
        "modelId": "Samantha-13B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "robin-13b-v2-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "robin-13b-v2-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "robin-13b-v2-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "robin-13b-v2-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "robin-13b-v2-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "robin-13b-v2-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "robin-13b-v2-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "robin-13b-v2-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "robin-13b-v2-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Robin-13B-v2-SuperHOT-8K-GGML",
        "modelId": "Robin-13B-v2-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "pygmalion-13b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "pygmalion-13b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "pygmalion-13b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "pygmalion-13b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "pygmalion-13b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "pygmalion-13b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "pygmalion-13b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "pygmalion-13b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "pygmalion-13b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Pygmalion-13B-SuperHOT-8K-GGML",
        "modelId": "Pygmalion-13B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "platypus-30b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "platypus-30b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "platypus-30b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "platypus-30b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "platypus-30b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "platypus-30b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "platypus-30b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "platypus-30b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "platypus-30b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Platypus-30B-SuperHOT-8K-GGML",
        "modelId": "Platypus-30B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "minotaur-13b-fixed-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "minotaur-13b-fixed-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "minotaur-13b-fixed-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "minotaur-13b-fixed-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "minotaur-13b-fixed-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "minotaur-13b-fixed-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "minotaur-13b-fixed-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "minotaur-13b-fixed-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "minotaur-13b-fixed-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Minotaur-13B-fixed-SuperHOT-8K-GGML",
        "modelId": "Minotaur-13B-fixed-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "manticore-13b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "manticore-13b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "manticore-13b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "manticore-13b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "manticore-13b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "manticore-13b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "manticore-13b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "manticore-13b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "manticore-13b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Manticore-13B-SuperHOT-8K-GGML",
        "modelId": "Manticore-13B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "manticore-13b-chat-pyg-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "manticore-13b-chat-pyg-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "manticore-13b-chat-pyg-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "manticore-13b-chat-pyg-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "manticore-13b-chat-pyg-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "manticore-13b-chat-pyg-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "manticore-13b-chat-pyg-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "manticore-13b-chat-pyg-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "manticore-13b-chat-pyg-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Manticore-13B-Chat-Pyg-SuperHOT-8K-GGML",
        "modelId": "Manticore-13B-Chat-Pyg-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "manticore-13b-chat-pyg-guanaco-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "manticore-13b-chat-pyg-guanaco-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "manticore-13b-chat-pyg-guanaco-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "manticore-13b-chat-pyg-guanaco-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "manticore-13b-chat-pyg-guanaco-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "manticore-13b-chat-pyg-guanaco-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "manticore-13b-chat-pyg-guanaco-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "manticore-13b-chat-pyg-guanaco-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "manticore-13b-chat-pyg-guanaco-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GGML",
        "modelId": "Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-30b-supercot-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-30b-supercot-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-30b-supercot-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-30b-supercot-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-30b-supercot-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-30b-supercot-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-30b-supercot-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-30b-supercot-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-30b-supercot-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/llama-30b-supercot-SuperHOT-8K-GGML",
        "modelId": "llama-30b-supercot-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "ultralm-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "ultralm-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "ultralm-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "ultralm-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "ultralm-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/UltraLM-13B-GGML",
        "modelId": "UltraLM-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "gpt4all-snoozy-13b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "gpt4all-snoozy-13b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "gpt4all-snoozy-13b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "gpt4all-snoozy-13b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "gpt4all-snoozy-13b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "gpt4all-snoozy-13b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "gpt4all-snoozy-13b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "gpt4all-snoozy-13b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "gpt4all-snoozy-13b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/GPT4All-13B-Snoozy-SuperHOT-8K-GGML",
        "modelId": "GPT4All-13B-Snoozy-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronos-hermes-13b-superhot-8k.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronos-hermes-13b-superhot-8k.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronos-hermes-13b-superhot-8k.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronos-hermes-13b-superhot-8k.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronos-hermes-13b-superhot-8k.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Chronos-Hermes-13B-SuperHOT-8K-GGML",
        "modelId": "Chronos-Hermes-13B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "camel-13b-role-playing-data-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "camel-13b-role-playing-data-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "camel-13b-role-playing-data-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "camel-13b-role-playing-data-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "camel-13b-role-playing-data-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "camel-13b-role-playing-data-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "camel-13b-role-playing-data-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "camel-13b-role-playing-data-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "camel-13b-role-playing-data-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/CAMEL-13B-Role-Playing-Data-SuperHOT-8K-GGML",
        "modelId": "CAMEL-13B-Role-Playing-Data-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "gplatty-30b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "gplatty-30b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "gplatty-30b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "gplatty-30b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "gplatty-30b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "gplatty-30b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "gplatty-30b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "gplatty-30b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "gplatty-30b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/GPlatty-30B-SuperHOT-8K-GGML",
        "modelId": "GPlatty-30B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-1.4.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/airoboros-65B-gpt4-1.4-GGML",
        "modelId": "airoboros-65B-gpt4-1.4-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "platypus-30b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "platypus-30b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "platypus-30b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "platypus-30b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "platypus-30b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "platypus-30b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "platypus-30b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "platypus-30b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "platypus-30b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "platypus-30b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "platypus-30b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "platypus-30b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "platypus-30b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "platypus-30b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Platypus-30B-GGML",
        "modelId": "Platypus-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-33b-gpt4-1.4-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/airoboros-33B-gpt4-1-4-SuperHOT-8K-GGML",
        "modelId": "airoboros-33B-gpt4-1-4-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronos-13b-superhot-8k.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronos-13b-superhot-8k.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronos-13b-superhot-8k.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronos-13b-superhot-8k.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronos-13b-superhot-8k.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronos-13b-superhot-8k.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronos-13b-superhot-8k.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronos-13b-superhot-8k.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronos-13b-superhot-8k.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Chronos-13B-SuperHOT-8K-GGML",
        "modelId": "Chronos-13B-SuperHOT-8K-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "gplatty-30b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "gplatty-30b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/GPlatty-30B-GGML",
        "modelId": "GPlatty-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-33b-gpt4-1.4.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-33B-gpt4-1.4-GGML",
        "modelId": "airoboros-33B-gpt4-1.4-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mpt-30b-chat.ggmlv0.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mpt-30b-chat.ggmlv0.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mpt-30b-chat.ggmlv0.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mpt-30b-chat.ggmlv0.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mpt-30b-chat.ggmlv0.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/mpt-30B-chat-GGML",
        "modelId": "mpt-30B-chat-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-mpt-30b-gpt4.ggmlv0.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-mpt-30b-gpt4.ggmlv0.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-mpt-30b-gpt4.ggmlv0.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-mpt-30b-gpt4.ggmlv0.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-mpt-30b-gpt4.ggmlv0.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-mpt-30b-gpt4-1p4-GGML",
        "modelId": "airoboros-mpt-30b-gpt4-1p4-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "selfee-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "selfee-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "selfee-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "selfee-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "selfee-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "selfee-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "selfee-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "selfee-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "selfee-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "selfee-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "selfee-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "selfee-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "selfee-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "selfee-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Selfee-13B-GGML",
        "modelId": "Selfee-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-13b-v1.3.0.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/vicuna-13b-v1.3.0-GGML",
        "modelId": "vicuna-13b-v1.3.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "orca-mini-3b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "orca-mini-3b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "orca-mini-3b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "orca-mini-3b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "orca-mini-3b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/orca_mini_3B-GGML",
        "modelId": "orca_mini_3B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "orca-mini-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "orca-mini-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/orca_mini_7B-GGML",
        "modelId": "orca_mini_7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "orca-mini-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "orca-mini-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/orca_mini_13B-GGML",
        "modelId": "orca_mini_13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizardlm-33b-v1.0-uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-33B-V1.0-Uncensored-GGML",
        "modelId": "WizardLM-33B-V1.0-Uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "open-llama-13b-open-instruct.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/open-llama-13b-open-instruct-GGML",
        "modelId": "open-llama-13b-open-instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mpt-30b.ggmlv0.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mpt-30b.ggmlv0.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mpt-30b.ggmlv0.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mpt-30b.ggmlv0.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mpt-30b.ggmlv0.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/mpt-30B-GGML",
        "modelId": "mpt-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mpt-30b-instruct.ggmlv0.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mpt-30b-instruct.ggmlv0.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mpt-30b-instruct.ggmlv0.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mpt-30b-instruct.ggmlv0.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mpt-30b-instruct.ggmlv0.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/mpt-30B-instruct-GGML",
        "modelId": "mpt-30B-instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardLM-7B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardLM-7B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardLM-7B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardLM-7B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardLM-7B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/wizardLM-7B-GGML",
        "modelId": "wizardLM-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "flan-openllama-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "flan-openllama-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Flan-OpenLlama-7B-GGML",
        "modelId": "Flan-OpenLlama-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-13b-gpt4-1.4.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-13B-gpt4-1.4-GGML",
        "modelId": "airoboros-13B-gpt4-1.4-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-7b-gpt4-1.4.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-7B-gpt4-1.4-GGML",
        "modelId": "airoboros-7B-gpt4-1.4-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "PMC_LLAMA-7B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/PMC_LLAMA-7B-GGML",
        "modelId": "PMC_LLAMA-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "guanaco-33B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "guanaco-33B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/guanaco-33B-GGML",
        "modelId": "guanaco-33B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "guanaco-13B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "guanaco-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/guanaco-13B-GGML",
        "modelId": "guanaco-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "guanaco-65B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "guanaco-65B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/guanaco-65B-GGML",
        "modelId": "guanaco-65B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-65b-gpt4-1.3.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/airoboros-65B-gpt4-1.3-GGML",
        "modelId": "airoboros-65B-gpt4-1.3-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-13b-gpt4-1.3.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-13B-gpt4-1.3-GGML",
        "modelId": "airoboros-13B-gpt4-1.3-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-7b-gpt4-1.3.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-7B-gpt4-1.3-GGML",
        "modelId": "airoboros-7B-gpt4-1.3-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-33b-gpt4-1.3.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-33B-gpt4-1.3-GGML",
        "modelId": "airoboros-33B-gpt4-1.3-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "baichuan-llama-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "baichuan-llama-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/baichuan-llama-7B-GGML",
        "modelId": "baichuan-llama-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "baichuan-vicuna-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/baichuan-vicuna-7B-GGML",
        "modelId": "baichuan-vicuna-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "cassandra-6.9b.ggmlv0.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "cassandra-6.9b.ggmlv0.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "cassandra-6.9b.ggmlv0.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "cassandra-6.9b.ggmlv0.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "cassandra-6.9b.ggmlv0.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/cassandra-6.9B-GGML",
        "modelId": "cassandra-6.9B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "minotaur-15b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "minotaur-15b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "minotaur-15b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "minotaur-15b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "minotaur-15b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/minotaur-15B-GGML",
        "modelId": "minotaur-15B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-7b-v1.3.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/vicuna-7B-v1.3-GGML",
        "modelId": "vicuna-7B-v1.3-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "gpt4-x-vicuna-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/gpt4-x-vicuna-13B-GGML",
        "modelId": "gpt4-x-vicuna-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizardlm-7b-v1.0-uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-7B-V1.0-Uncensored-GGML",
        "modelId": "WizardLM-7B-V1.0-Uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "gpt4-x-alpaca-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/gpt4-x-alpaca-13B-GGML",
        "modelId": "gpt4-x-alpaca-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "camel-33B-combined-data.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "camel-33B-combined-data.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/CAMEL-33B-Combined-Data-GGML",
        "modelId": "CAMEL-33B-Combined-Data-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "robin-65b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "robin-65b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "robin-65b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "robin-65b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "robin-65b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "robin-65b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "robin-65b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "robin-65b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "robin-65b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "robin-65b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "robin-65b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "robin-65b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/robin-65B-v2-GGML",
        "modelId": "robin-65B-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "robin-33b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "robin-33b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "robin-33b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "robin-33b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "robin-33b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "robin-33b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "robin-33b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "robin-33b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "robin-33b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "robin-33b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "robin-33b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "robin-33b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "robin-33b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "robin-33b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/robin-33B-v2-GGML",
        "modelId": "robin-33B-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "robin-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "robin-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "robin-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "robin-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "robin-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "robin-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "robin-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "robin-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "robin-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "robin-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "robin-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "robin-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "robin-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "robin-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/robin-13B-v2-GGML",
        "modelId": "robin-13B-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "robin-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "robin-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "robin-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "robin-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "robin-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "robin-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "robin-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "robin-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "robin-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "robin-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "robin-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "robin-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "robin-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "robin-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/robin-7B-v2-GGML",
        "modelId": "robin-7B-v2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-13b-gpt4-1.2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-13B-gpt4-1.2-GGML",
        "modelId": "airoboros-13B-gpt4-1.2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-7b-gpt4-1.2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-7B-gpt4-1.2-GGML",
        "modelId": "airoboros-7B-gpt4-1.2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "WizardCoder-15B-1.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "WizardCoder-15B-1.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "WizardCoder-15B-1.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "WizardCoder-15B-1.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "WizardCoder-15B-1.0.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardCoder-15B-1.0-GGML",
        "modelId": "WizardCoder-15B-1.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "starchat-beta.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "starchat-beta.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "starchat-beta.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "starchat-beta.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "starchat-beta.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/starchat-beta-GGML",
        "modelId": "starchat-beta-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-65B-gpt4-1.2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/airoboros-65B-gpt4-1.2-GGML",
        "modelId": "airoboros-65B-gpt4-1.2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-33b-gpt4-1.2.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-33B-gpt4-1.2-GGML",
        "modelId": "airoboros-33B-gpt4-1.2-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "minotaur-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "minotaur-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/minotaur-13B-fixed-GGML",
        "modelId": "minotaur-13B-fixed-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "camel-13b-roleplay.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "camel-13b-roleplay.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/CAMEL-13B-Role-Playing-Data-GGML",
        "modelId": "CAMEL-13B-Role-Playing-Data-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "camel-13b-combined.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "camel-13b-combined.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/CAMEL-13B-Combined-Data-GGML",
        "modelId": "CAMEL-13B-Combined-Data-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "tulu-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "tulu-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "tulu-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "tulu-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "tulu-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "tulu-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "tulu-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "tulu-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "tulu-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "tulu-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "tulu-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "tulu-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "tulu-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "tulu-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/tulu-7B-GGML",
        "modelId": "tulu-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "tulu-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "tulu-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "tulu-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "tulu-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "tulu-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "tulu-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "tulu-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "tulu-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "tulu-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "tulu-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "tulu-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "tulu-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "tulu-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "tulu-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/tulu-13B-GGML",
        "modelId": "tulu-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "tulu-30b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "tulu-30b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "tulu-30b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "tulu-30b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "tulu-30b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "tulu-30b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "tulu-30b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "tulu-30b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "tulu-30b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "tulu-30b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "tulu-30b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "tulu-30b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "tulu-30b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "tulu-30b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/tulu-30B-GGML",
        "modelId": "tulu-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "open-llama-7B-open-instruct.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/open-llama-7b-open-instruct-GGML",
        "modelId": "open-llama-7b-open-instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "fin-llama-33b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "fin-llama-33b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/fin-llama-33B-GGML",
        "modelId": "fin-llama-33B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-33b-gpt4.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-33b-gpt4-GGML",
        "modelId": "airoboros-33b-gpt4-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "samantha-1.1-llama-33b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/samantha-1.1-llama-33B-GGML",
        "modelId": "samantha-1.1-llama-33B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "samantha-1.1-llama-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/samantha-1.1-llama-13B-GGML",
        "modelId": "samantha-1.1-llama-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-13b-1.1.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-13B-1.1-GGML",
        "modelId": "airoboros-13B-1.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "samantha-1.1-llama-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/samantha-1.1-llama-7B-GGML",
        "modelId": "samantha-1.1-llama-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "Gorilla-7B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "Gorilla-7B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/gorilla-7B-GGML",
        "modelId": "gorilla-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "alpaca-lora-65B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "alpaca-lora-65B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/alpaca-lora-65B-GGML",
        "modelId": "alpaca-lora-65B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "gpt4-alpaca-lora_mlp-65B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/gpt4-alpaca-lora_mlp-65B-GGML",
        "modelId": "gpt4-alpaca-lora_mlp-65B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "OpenAssistant-SFT-7-Llama-30B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/OpenAssistant-SFT-7-Llama-30B-GGML",
        "modelId": "OpenAssistant-SFT-7-Llama-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "WizardLM-13B-1.0.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/wizardLM-13B-1.0-GGML",
        "modelId": "wizardLM-13B-1.0-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "Vigogne-Instruct-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Vigogne-Instruct-13B-GGML",
        "modelId": "Vigogne-Instruct-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "Manticore-13B-Chat-Pyg.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/manticore-13b-chat-pyg-GGML",
        "modelId": "manticore-13b-chat-pyg-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-30b-supercot.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "llama-30b-supercot.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/llama-30b-supercot-GGML",
        "modelId": "llama-30b-supercot-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "h2ogptq-oasst1-512-30B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/h2ogpt-oasst1-512-30B-GGML",
        "modelId": "h2ogpt-oasst1-512-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "VicUnlocked-30B-LoRA.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/VicUnlocked-30B-LoRA-GGML",
        "modelId": "VicUnlocked-30B-LoRA-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "stable-vicuna-13B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "stable-vicuna-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/stable-vicuna-13B-GGML",
        "modelId": "stable-vicuna-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "koala-7B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "koala-7B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "koala-7B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "koala-7B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "koala-7B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "koala-7B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "koala-7B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "koala-7B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "koala-7B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "koala-7B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "koala-7B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "koala-7B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "koala-7B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "koala-7B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/koala-7B-GGML",
        "modelId": "koala-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "13B-HyperMantis.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "13B-HyperMantis.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/13B-HyperMantis-GGML",
        "modelId": "13B-HyperMantis-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "koala-13B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "koala-13B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "koala-13B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "koala-13B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "koala-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "koala-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "koala-13B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "koala-13B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "koala-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "koala-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "koala-13B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "koala-13B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "koala-13B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "koala-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/koala-13B-GGML",
        "modelId": "koala-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "Karen-The-Editor.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "Karen-The-Editor.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Karen_theEditor_13B-GGML",
        "modelId": "Karen_theEditor_13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "WizardLM-7B-uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-7B-uncensored-GGML",
        "modelId": "WizardLM-7B-uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "based-30b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "based-30b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "based-30b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "based-30b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "based-30b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "based-30b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "based-30b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "based-30b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "based-30b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "based-30b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "based-30b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "based-30b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "based-30b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "based-30b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/based-30B-GGML",
        "modelId": "based-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizard-vicuna-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/wizard-vicuna-13B-GGML",
        "modelId": "wizard-vicuna-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "based-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "based-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "based-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "based-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "based-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "based-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "based-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "based-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "based-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "based-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "based-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "based-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "based-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "based-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/based-13b-GGML",
        "modelId": "based-13b-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "based-7B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "based-7B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "based-7B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "based-7B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "based-7B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            },
            {
                "filename": "based-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "based-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "based-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "based-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "based-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "based-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "based-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "based-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "based-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/based-7B-GGML",
        "modelId": "based-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "Wizard-Vicuna-7B-Uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Wizard-Vicuna-7B-Uncensored-GGML",
        "modelId": "Wizard-Vicuna-7B-Uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronos-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronos-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronos-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronos-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronos-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronos-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronos-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronos-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronos-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronos-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronos-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronos-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronos-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronos-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/chronos-13B-GGML",
        "modelId": "chronos-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "hippogriff-30b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "hippogriff-30b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/hippogriff-30b-chat-GGML",
        "modelId": "hippogriff-30b-chat-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizardLM-13B-Uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-13B-Uncensored-GGML",
        "modelId": "WizardLM-13B-Uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "starcoder.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "starcoder.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "starcoder.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "starcoder.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "starcoder.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/starcoder-GGML",
        "modelId": "starcoder-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "starcoderplus.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "starcoderplus.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "starcoderplus.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "starcoderplus.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "starcoderplus.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/starcoderplus-GGML",
        "modelId": "starcoderplus-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "minotaur-13B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "minotaur-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/minotaur-13B-GGML",
        "modelId": "minotaur-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "minotaur-mpt-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "minotaur-mpt-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "minotaur-mpt-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "minotaur-mpt-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "minotaur-mpt-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/minotaur-mpt-7B-GGML",
        "modelId": "minotaur-mpt-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-7B-cot.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-7B-cot.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Vicuna-7B-CoT-GGML",
        "modelId": "Vicuna-7B-CoT-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-13b-cot.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-13b-cot.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Vicuna-13B-CoT-GGML",
        "modelId": "Vicuna-13B-CoT-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "selfee-7B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "selfee-7B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "selfee-7B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "selfee-7B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "selfee-7B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "selfee-7B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "selfee-7B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "selfee-7B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "selfee-7B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "selfee-7B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "selfee-7B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "selfee-7B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "selfee-7B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "selfee-7B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/selfee-7B-GGML",
        "modelId": "selfee-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mpt-7b-storywriter.ggmlv3.fp16.bin",
                "quantization": "fp16"
            },
            {
                "filename": "mpt-7b-storywriter.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mpt-7b-storywriter.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mpt-7b-storywriter.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mpt-7b-storywriter.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mpt-7b-storywriter.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/MPT-7B-Storywriter-GGML",
        "modelId": "MPT-7B-Storywriter-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mpt-7b-instruct.ggmlv3.fp16.bin",
                "quantization": "fp16"
            },
            {
                "filename": "mpt-7b-instruct.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mpt-7b-instruct.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mpt-7b-instruct.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mpt-7b-instruct.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mpt-7b-instruct.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/MPT-7B-Instruct-GGML",
        "modelId": "MPT-7B-Instruct-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "mpt-7b.ggmlv3.fp16.bin",
                "quantization": "fp16"
            },
            {
                "filename": "mpt-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "mpt-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "mpt-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "mpt-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "mpt-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/MPT-7B-GGML",
        "modelId": "MPT-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "GPT4All-13B-snoozy.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/GPT4All-13B-snoozy-GGML",
        "modelId": "GPT4All-13B-snoozy-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-7B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-7B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-7B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-7B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-7B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-7B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-7B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-7B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-7B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "airoboros-7b-gpt4.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-7b-gpt4.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-7b-gpt4.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-7b-gpt4.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-7b-gpt4.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/airoboros-7b-gpt4-GGML",
        "modelId": "airoboros-7b-gpt4-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "airoboros-13b-gpt4.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "airoboros-13b-gpt4.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "airoboros-13b-gpt4.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "airoboros-13b-gpt4.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "airoboros-13b-gpt4.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            },
            {
                "filename": "airoboros-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "airoboros-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "airoboros-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "airoboros-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "airoboros-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "airoboros-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "airoboros-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "airoboros-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "airoboros-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/airoboros-13b-gpt4-GGML",
        "modelId": "airoboros-13b-gpt4-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronos-33b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronos-33b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronos-33b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronos-33b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronos-33b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronos-33b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronos-33b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronos-33b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronos-33b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronos-33b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronos-33b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronos-33b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronos-33b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronos-33b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/chronos-33b-GGML",
        "modelId": "chronos-33b-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "Samantha-7B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "Samantha-7B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "Samantha-7B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "Samantha-7B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "Samantha-7B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            },
            {
                "filename": "samantha-7B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "samantha-7B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "samantha-7B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "samantha-7B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "samantha-7B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "samantha-7B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "samantha-7B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "samantha-7B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "samantha-7B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/Samantha-7B-GGML",
        "modelId": "Samantha-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-13B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "samantha-13B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "samantha-13B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "samantha-13B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "samantha-13B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "samantha-13B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "samantha-13B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "samantha-13B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "samantha-13B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "samantha-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "samantha-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "samantha-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "samantha-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "samantha-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/samantha-13B-GGML",
        "modelId": "samantha-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Wizard-Vicuna-13B-Uncensored-GGML",
        "modelId": "Wizard-Vicuna-13B-Uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-13b-1.1.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/vicuna-13b-1.1-GGML",
        "modelId": "vicuna-13b-1.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "vicuna-7b-1.1.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/vicuna-7B-1.1-GGML",
        "modelId": "vicuna-7B-1.1-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "30b-Lazarus.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "30b-Lazarus.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/30B-Lazarus-GGML",
        "modelId": "30B-Lazarus-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "Manticore-13B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "Manticore-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Manticore-13B-GGML",
        "modelId": "Manticore-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "chronos-wizardlm-uc-scot-st-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/chronos-wizardlm-uc-scot-st-13B-GGML",
        "modelId": "chronos-wizardlm-uc-scot-st-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GGML",
        "modelId": "WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "VicUnlocked-Alpaca-65B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "VicUnlocked-Alpaca-65B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "VicUnlocked-Alpaca-65B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "VicUnlocked-Alpaca-65B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "vicunlocked-65b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "vicunlocked-65b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "vicunlocked-65b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "vicunlocked-65b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "vicunlocked-65b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "vicunlocked-65b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "vicunlocked-65b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "vicunlocked-65b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/VicUnlocked-alpaca-65B-QLoRA-GGML",
        "modelId": "VicUnlocked-alpaca-65B-QLoRA-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "dromedary-lora-65B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "dromedary-lora-65B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            }
        ],
        "id": "TheBloke/dromedary-65B-lora-GGML",
        "modelId": "dromedary-65B-lora-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "WizardLM-30B-Uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "WizardLM-30B-Uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "WizardLM-30B-Uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "WizardLM-30B-Uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "WizardLM-30B-Uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            },
            {
                "filename": "wizardlm-30b-uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-30b-uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-30b-uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-30b-uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-30b-uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-30b-uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-30b-uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-30b-uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardlm-30b-uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            }
        ],
        "id": "TheBloke/WizardLM-30B-Uncensored-GGML",
        "modelId": "WizardLM-30B-Uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "Wizard-Vicuna-30B-Uncensored.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Wizard-Vicuna-30B-Uncensored-GGML",
        "modelId": "Wizard-Vicuna-30B-Uncensored-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "13b-chimera.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "13b-chimera.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "13b-chimera.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "13b-chimera.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "13b-chimera.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "13b-chimera.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "13b-chimera.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "13b-chimera.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "13b-chimera.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "13b-chimera.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "13b-chimera.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "13b-chimera.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "13b-chimera.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "13b-chimera.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/13B-Chimera-GGML",
        "modelId": "13B-Chimera-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "nous-hermes-13b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "nous-hermes-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Nous-Hermes-13B-GGML",
        "modelId": "Nous-Hermes-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizardlm-30b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "wizardlm-30b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/WizardLM-30B-GGML",
        "modelId": "WizardLM-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "planner-7b.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "planner-7b.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "planner-7b.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "planner-7b.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "planner-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "planner-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "planner-7b.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "planner-7b.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "planner-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "planner-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "planner-7b.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "planner-7b.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "planner-7b.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "planner-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Planner-7B-GGML",
        "modelId": "Planner-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "samantha-33B.ggmlv3.q2_K.bin",
                "quantization": "q2_K"
            },
            {
                "filename": "samantha-33B.ggmlv3.q3_K_L.bin",
                "quantization": "q3_K_L"
            },
            {
                "filename": "samantha-33B.ggmlv3.q3_K_M.bin",
                "quantization": "q3_K_M"
            },
            {
                "filename": "samantha-33B.ggmlv3.q3_K_S.bin",
                "quantization": "q3_K_S"
            },
            {
                "filename": "samantha-33B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "samantha-33B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "samantha-33B.ggmlv3.q4_K_M.bin",
                "quantization": "q4_K_M"
            },
            {
                "filename": "samantha-33B.ggmlv3.q4_K_S.bin",
                "quantization": "q4_K_S"
            },
            {
                "filename": "samantha-33B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "samantha-33B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "samantha-33B.ggmlv3.q5_K_M.bin",
                "quantization": "q5_K_M"
            },
            {
                "filename": "samantha-33B.ggmlv3.q5_K_S.bin",
                "quantization": "q5_K_S"
            },
            {
                "filename": "samantha-33B.ggmlv3.q6_K.bin",
                "quantization": "q6_K"
            },
            {
                "filename": "samantha-33B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/samantha-33B-GGML",
        "modelId": "samantha-33B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-13b-supercot.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-13b-supercot.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-13b-supercot.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-13b-supercot.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-13b-supercot.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/llama-13b-supercot-GGML",
        "modelId": "llama-13b-supercot-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "llama-deus-7b-v3.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "llama-deus-7b-v3.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "llama-deus-7b-v3.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "llama-deus-7b-v3.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "llama-deus-7b-v3.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/llama-deus-7b-v3-GGML",
        "modelId": "llama-deus-7b-v3-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "Alpaca-Lora-30B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "Alpaca-Lora-30B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "Alpaca-Lora-30B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "Alpaca-Lora-30B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "Alpaca-Lora-30B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Alpaca-Lora-30B-GGML",
        "modelId": "Alpaca-Lora-30B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "baize-v2-13b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "baize-v2-13b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "baize-v2-13b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "baize-v2-13b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "baize-v2-13b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Project-Baize-v2-13B-GGML",
        "modelId": "Project-Baize-v2-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "baize-v2-7b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "baize-v2-7b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "baize-v2-7b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "baize-v2-7b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "baize-v2-7b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/Project-Baize-v2-7B-GGML",
        "modelId": "Project-Baize-v2-7B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "medalpaca-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "medalpaca-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "medalpaca-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "medalpaca-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "medalpaca-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/medalpaca-13B-GGML",
        "modelId": "medalpaca-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "wizard-mega-13B.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "wizard-mega-13B.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "wizard-mega-13B.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "wizard-mega-13B.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "wizard-mega-13B.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/wizard-mega-13B-GGML",
        "modelId": "wizard-mega-13B-GGML",
        "modelRepo": "TheBloke"
    },
    {
        "files": [
            {
                "filename": "gpt4-alpaca-lora-30b.ggmlv3.q4_0.bin",
                "quantization": "q4_0"
            },
            {
                "filename": "gpt4-alpaca-lora-30b.ggmlv3.q4_1.bin",
                "quantization": "q4_1"
            },
            {
                "filename": "gpt4-alpaca-lora-30b.ggmlv3.q5_0.bin",
                "quantization": "q5_0"
            },
            {
                "filename": "gpt4-alpaca-lora-30b.ggmlv3.q5_1.bin",
                "quantization": "q5_1"
            },
            {
                "filename": "gpt4-alpaca-lora-30b.ggmlv3.q8_0.bin",
                "quantization": "q8_0"
            }
        ],
        "id": "TheBloke/gpt4-alpaca-lora-30B-GGML",
        "modelId": "gpt4-alpaca-lora-30B-GGML",
        "modelRepo": "TheBloke"
    }
]